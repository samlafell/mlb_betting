#!/usr/bin/env python3
"""
Three-Tier Pipeline Validation Tests

Comprehensive testing framework for validating the RAW â†’ STAGING â†’ CURATED
data pipeline architecture. This replaces any legacy core_betting schema testing.

Test Phases:
- Phase 1: Database Schema Validation
- Phase 2: Individual Source Testing (Action Network, SBD, VSIN)
- Phase 3: Cross-Source Integration Testing
- Phase 4: Pipeline Processing Testing
- Phase 5: End-to-End Pipeline Testing
- Phase 6: Performance and Error Testing
"""

import asyncio
import json
import os
import uuid
from datetime import datetime
from typing import Any

import pytest
import structlog
from asyncpg import Connection, Pool, create_pool

from src.core.config import get_settings
from src.data.collection.consolidated_action_network_collector import (
    ActionNetworkCollector,
)
from src.data.collection.sbd_unified_collector_api import SBDUnifiedCollectorAPI
from src.data.collection.vsin_unified_collector import VSINUnifiedCollector

logger = structlog.get_logger(__name__)


class ThreeTierPipelineValidator:
    """Comprehensive validator for the three-tier data pipeline."""

    def __init__(self):
        self.config = get_settings()
        self.db_pool: Pool | None = None
        self.execution_id = str(uuid.uuid4())
        self.test_results = {
            "execution_id": self.execution_id,
            "timestamp": datetime.now().isoformat(),
            "phases": {},
        }

    async def __aenter__(self):
        """Async context manager entry."""
        await self.initialize_database()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        if self.db_pool:
            await self.db_pool.close()

    async def initialize_database(self):
        """Initialize database connection pool."""
        try:
            database_url = os.getenv(
                "DATABASE_URL", "postgresql://samlafell@localhost:5432/mlb_betting"
            )
            self.db_pool = await create_pool(
                database_url, min_size=2, max_size=10, command_timeout=60
            )
            logger.info("Database connection pool initialized")
        except Exception as e:
            logger.error(f"Failed to initialize database: {str(e)}")
            raise

    async def log_pipeline_execution(
        self, stage: str, status: str, metadata: dict | None = None
    ):
        """Log pipeline execution to tracking table."""
        if not self.db_pool:
            return

        try:
            async with self.db_pool.acquire() as conn:
                await conn.execute(
                    """
                    INSERT INTO public.pipeline_execution_log 
                    (execution_id, pipeline_stage, start_time, status, metadata)
                    VALUES ($1, $2, $3, $4, $5)
                    """,
                    self.execution_id,
                    stage,
                    datetime.now(),
                    status,
                    json.dumps(metadata or {}),
                )
        except Exception as e:
            logger.error(f"Failed to log pipeline execution: {str(e)}")

    # ==============================
    # Phase 1: Database Schema Validation
    # ==============================

    async def validate_phase_1_schemas(self) -> dict[str, Any]:
        """
        Phase 1: Validate that the three-tier pipeline schemas exist.

        Tests:
        1. Verify raw_data, staging, curated schemas exist
        2. Validate source-specific raw tables exist
        3. Verify staging unified tables exist
        4. Check proper indexes and constraints
        """
        logger.info("Starting Phase 1: Database Schema Validation")
        await self.log_pipeline_execution("phase_1_start", "running")

        phase_results = {
            "phase": "1_schema_validation",
            "status": "running",
            "tests": {},
            "start_time": datetime.now().isoformat(),
        }

        try:
            async with self.db_pool.acquire() as conn:
                # Test 1.1: Verify pipeline schemas exist
                schema_results = await self._test_pipeline_schemas(conn)
                phase_results["tests"]["pipeline_schemas"] = schema_results

                # Test 1.2: Verify raw_data tables exist
                raw_table_results = await self._test_raw_data_tables(conn)
                phase_results["tests"]["raw_data_tables"] = raw_table_results

                # Test 1.3: Verify staging tables exist
                staging_results = await self._test_staging_tables(conn)
                phase_results["tests"]["staging_tables"] = staging_results

                # Test 1.4: Verify indexes and constraints
                index_results = await self._test_indexes_constraints(conn)
                phase_results["tests"]["indexes_constraints"] = index_results

                # Test 1.5: Verify pipeline execution log table
                execution_log_results = await self._test_execution_log_table(conn)
                phase_results["tests"]["execution_log"] = execution_log_results

            # Determine overall phase status
            all_tests_passed = all(
                test["status"] == "passed" for test in phase_results["tests"].values()
            )
            phase_results["status"] = "passed" if all_tests_passed else "failed"
            phase_results["end_time"] = datetime.now().isoformat()

            await self.log_pipeline_execution(
                "phase_1_complete", phase_results["status"], phase_results
            )

            logger.info(
                f"Phase 1 completed: {phase_results['status']}",
                passed_tests=sum(
                    1
                    for t in phase_results["tests"].values()
                    if t["status"] == "passed"
                ),
                total_tests=len(phase_results["tests"]),
            )

            return phase_results

        except Exception as e:
            phase_results["status"] = "error"
            phase_results["error"] = str(e)
            phase_results["end_time"] = datetime.now().isoformat()

            await self.log_pipeline_execution("phase_1_error", "failed", phase_results)
            logger.error(f"Phase 1 failed with error: {str(e)}")

            return phase_results

    async def _test_pipeline_schemas(self, conn: Connection) -> dict[str, Any]:
        """Test 1.1: Verify pipeline schemas exist."""
        try:
            # Check for required schemas
            required_schemas = ["raw_data", "staging", "curated"]
            existing_schemas = await conn.fetch(
                """
                SELECT schema_name 
                FROM information_schema.schemata 
                WHERE schema_name = ANY($1)
                """,
                required_schemas,
            )

            existing_schema_names = [row["schema_name"] for row in existing_schemas]
            missing_schemas = set(required_schemas) - set(existing_schema_names)

            result = {
                "status": "passed" if not missing_schemas else "failed",
                "required_schemas": required_schemas,
                "existing_schemas": existing_schema_names,
                "missing_schemas": list(missing_schemas),
                "test_name": "Pipeline Schemas Validation",
            }

            if missing_schemas:
                result["error"] = f"Missing schemas: {missing_schemas}"

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "test_name": "Pipeline Schemas Validation",
            }

    async def _test_raw_data_tables(self, conn: Connection) -> dict[str, Any]:
        """Test 1.2: Verify source-specific raw_data tables exist."""
        try:
            # Expected raw_data tables based on the current collectors
            expected_raw_tables = [
                "raw_data.action_network_odds",
                "raw_data.action_network_history",
                "raw_data.sbd_betting_splits",
                "raw_data.vsin_data",  # Corrected table name
                "raw_data.mlb_game_outcomes",  # From recent outcome checking work
            ]

            existing_tables = await conn.fetch(
                """
                SELECT schemaname, tablename,
                       schemaname || '.' || tablename as full_table_name
                FROM pg_tables 
                WHERE schemaname = 'raw_data'
                ORDER BY tablename
                """
            )

            existing_table_names = [row["full_table_name"] for row in existing_tables]
            missing_tables = set(expected_raw_tables) - set(existing_table_names)

            # Also check for any unexpected tables (might indicate legacy issues)
            unexpected_tables = set(existing_table_names) - set(expected_raw_tables)

            result = {
                "status": "passed" if not missing_tables else "failed",
                "expected_tables": expected_raw_tables,
                "existing_tables": existing_table_names,
                "missing_tables": list(missing_tables),
                "unexpected_tables": list(unexpected_tables),
                "test_name": "Raw Data Tables Validation",
            }

            if missing_tables:
                result["error"] = f"Missing raw_data tables: {missing_tables}"

            # Add warning for unexpected tables but don't fail the test
            if unexpected_tables:
                result["warning"] = f"Unexpected tables found: {unexpected_tables}"

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "test_name": "Raw Data Tables Validation",
            }

    async def _test_staging_tables(self, conn: Connection) -> dict[str, Any]:
        """Test 1.3: Verify staging unified tables exist."""
        try:
            # Expected staging tables from the migration
            expected_staging_tables = [
                "staging.action_network_games",
                "staging.action_network_odds_historical",
            ]

            existing_tables = await conn.fetch(
                """
                SELECT schemaname, tablename,
                       schemaname || '.' || tablename as full_table_name
                FROM pg_tables 
                WHERE schemaname = 'staging'
                ORDER BY tablename
                """
            )

            existing_table_names = [row["full_table_name"] for row in existing_tables]
            missing_tables = set(expected_staging_tables) - set(existing_table_names)

            # Check for any legacy core_betting references (should be none)
            legacy_check = await conn.fetch(
                """
                SELECT schemaname || '.' || tablename as full_table_name
                FROM pg_tables 
                WHERE schemaname = 'core_betting'
                """
            )

            legacy_tables = [row["full_table_name"] for row in legacy_check]

            result = {
                "status": "passed"
                if not missing_tables and not legacy_tables
                else "failed",
                "expected_tables": expected_staging_tables,
                "existing_tables": existing_table_names,
                "missing_tables": list(missing_tables),
                "legacy_tables_found": legacy_tables,
                "test_name": "Staging Tables Validation",
            }

            if missing_tables:
                result["error"] = f"Missing staging tables: {missing_tables}"

            if legacy_tables:
                result["error"] = (
                    f"Legacy core_betting tables found (should be migrated): {legacy_tables}"
                )

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "test_name": "Staging Tables Validation",
            }

    async def _test_indexes_constraints(self, conn: Connection) -> dict[str, Any]:
        """Test 1.4: Verify critical indexes and constraints exist."""
        try:
            # Check for critical indexes from the migration
            expected_indexes = [
                "idx_an_games_external_id",
                "idx_an_games_mlb_id",
                "idx_historical_odds_game_id",
                "idx_historical_odds_updated_at",
                "idx_pipeline_log_execution_id",
            ]

            existing_indexes = await conn.fetch(
                """
                SELECT indexname
                FROM pg_indexes 
                WHERE schemaname IN ('staging', 'public')
                AND indexname = ANY($1)
                """,
                expected_indexes,
            )

            existing_index_names = [row["indexname"] for row in existing_indexes]
            missing_indexes = set(expected_indexes) - set(existing_index_names)

            # Check key constraints on staging tables
            constraint_check = await conn.fetch(
                """
                SELECT conname, contype
                FROM pg_constraint c
                JOIN pg_class t ON c.conrelid = t.oid
                JOIN pg_namespace n ON t.relnamespace = n.oid
                WHERE n.nspname = 'staging'
                AND t.relname IN ('action_network_games', 'action_network_odds_historical')
                AND contype IN ('p', 'u', 'c')  -- primary, unique, check constraints
                """
            )

            constraint_names = [row["conname"] for row in constraint_check]

            result = {
                "status": "passed" if not missing_indexes else "failed",
                "expected_indexes": expected_indexes,
                "existing_indexes": existing_index_names,
                "missing_indexes": list(missing_indexes),
                "constraints_found": len(constraint_names),
                "constraint_details": constraint_names,
                "test_name": "Indexes and Constraints Validation",
            }

            if missing_indexes:
                result["error"] = f"Missing indexes: {missing_indexes}"

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "test_name": "Indexes and Constraints Validation",
            }

    async def _test_execution_log_table(self, conn: Connection) -> dict[str, Any]:
        """Test 1.5: Verify pipeline execution log table exists and is functional."""
        try:
            # Check if table exists
            table_exists = await conn.fetchval(
                """
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = 'pipeline_execution_log'
                )
                """
            )

            if not table_exists:
                return {
                    "status": "failed",
                    "error": "pipeline_execution_log table does not exist",
                    "test_name": "Execution Log Table Validation",
                }

            # Test basic functionality by inserting a test record
            test_execution_id = str(uuid.uuid4())
            await conn.execute(
                """
                INSERT INTO public.pipeline_execution_log 
                (execution_id, pipeline_stage, start_time, status, metadata)
                VALUES ($1, $2, $3, $4, $5)
                """,
                test_execution_id,
                "test_validation",
                datetime.now(),
                "completed",
                json.dumps({"test": True}),
            )

            # Verify the record was inserted
            inserted_record = await conn.fetchval(
                """
                SELECT COUNT(*) FROM public.pipeline_execution_log 
                WHERE execution_id = $1
                """,
                test_execution_id,
            )

            # Clean up test record
            await conn.execute(
                "DELETE FROM public.pipeline_execution_log WHERE execution_id = $1",
                test_execution_id,
            )

            result = {
                "status": "passed" if inserted_record > 0 else "failed",
                "table_exists": table_exists,
                "functional_test": inserted_record > 0,
                "test_name": "Execution Log Table Validation",
            }

            if inserted_record == 0:
                result["error"] = "Failed to insert/retrieve test record"

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "test_name": "Execution Log Table Validation",
            }

    # ==============================
    # Phase 2: Individual Source Testing
    # ==============================

    async def validate_phase_2_sources(self) -> dict[str, Any]:
        """
        Phase 2: Test individual data source collectors.

        Tests:
        1. Action Network data collection and raw storage
        2. SBD API data collection and raw storage
        3. VSIN data collection and raw storage
        4. Validate raw data format and completeness
        """
        logger.info("Starting Phase 2: Individual Source Testing")
        await self.log_pipeline_execution("phase_2_start", "running")

        phase_results = {
            "phase": "2_individual_sources",
            "status": "running",
            "tests": {},
            "start_time": datetime.now().isoformat(),
        }

        try:
            # Test 2.1: Action Network collection
            action_network_results = await self._test_action_network_collection()
            phase_results["tests"]["action_network"] = action_network_results

            # Test 2.2: SBD API collection
            sbd_results = await self._test_sbd_collection()
            phase_results["tests"]["sbd_api"] = sbd_results

            # Test 2.3: VSIN collection
            vsin_results = await self._test_vsin_collection()
            phase_results["tests"]["vsin"] = vsin_results

            # Test 2.4: Raw data validation
            raw_validation_results = await self._test_raw_data_validation()
            phase_results["tests"]["raw_data_validation"] = raw_validation_results

            # Determine overall phase status
            all_tests_passed = all(
                test["status"] == "passed" for test in phase_results["tests"].values()
            )
            phase_results["status"] = "passed" if all_tests_passed else "failed"
            phase_results["end_time"] = datetime.now().isoformat()

            await self.log_pipeline_execution(
                "phase_2_complete", phase_results["status"], phase_results
            )

            logger.info(
                f"Phase 2 completed: {phase_results['status']}",
                passed_tests=sum(
                    1
                    for t in phase_results["tests"].values()
                    if t["status"] == "passed"
                ),
                total_tests=len(phase_results["tests"]),
            )

            return phase_results

        except Exception as e:
            phase_results["status"] = "error"
            phase_results["error"] = str(e)
            phase_results["end_time"] = datetime.now().isoformat()

            await self.log_pipeline_execution("phase_2_error", "failed", phase_results)
            logger.error(f"Phase 2 failed with error: {str(e)}")

            return phase_results

    async def _test_action_network_collection(self) -> dict[str, Any]:
        """Test 2.1: Action Network data collection and raw storage."""
        try:
            collector = ActionNetworkCollector()

            # Test collection with a small sample
            test_result = collector.test_collection("mlb")

            # Verify raw data was stored
            async with self.db_pool.acquire() as conn:
                raw_count = await conn.fetchval(
                    """
                    SELECT COUNT(*) FROM raw_data.action_network_odds 
                    WHERE created_at >= NOW() - INTERVAL '5 minutes'
                    """
                )

            result = {
                "status": "passed"
                if test_result.get("status") == "success" and raw_count > 0
                else "failed",
                "collector_status": test_result.get("status"),
                "raw_records_stored": raw_count,
                "test_result": test_result,
                "test_name": "Action Network Collection Test",
            }

            if test_result.get("status") != "success":
                result["error"] = test_result.get("error", "Collection failed")
            elif raw_count == 0:
                result["error"] = "No records stored in raw_data.action_network_odds"

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "test_name": "Action Network Collection Test",
            }

    async def _test_sbd_collection(self) -> dict[str, Any]:
        """Test 2.2: SBD API data collection and raw storage."""
        try:
            collector = SBDUnifiedCollectorAPI()

            # Test collection
            test_result = collector.test_collection("mlb")

            # Verify raw data was stored
            async with self.db_pool.acquire() as conn:
                raw_count = await conn.fetchval(
                    """
                    SELECT COUNT(*) FROM raw_data.sbd_betting_splits 
                    WHERE created_at >= NOW() - INTERVAL '5 minutes'
                    """
                )

            result = {
                "status": "passed"
                if test_result.get("status") == "success" and raw_count > 0
                else "failed",
                "collector_status": test_result.get("status"),
                "raw_records_stored": raw_count,
                "test_result": test_result,
                "test_name": "SBD API Collection Test",
            }

            if test_result.get("status") != "success":
                result["error"] = test_result.get("error", "Collection failed")
            elif raw_count == 0:
                result["error"] = "No records stored in raw_data.sbd_betting_splits"

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "test_name": "SBD API Collection Test",
            }

    async def _test_vsin_collection(self) -> dict[str, Any]:
        """Test 2.3: VSIN data collection and raw storage."""
        try:
            collector = VSINUnifiedCollector()

            # Test collection
            test_result = collector.test_collection("mlb")

            # Verify raw data was stored
            async with self.db_pool.acquire() as conn:
                raw_count = await conn.fetchval(
                    """
                    SELECT COUNT(*) FROM raw_data.vsin_data 
                    WHERE created_at >= NOW() - INTERVAL '5 minutes'
                    """
                )

            result = {
                "status": "passed"
                if test_result.get("status") == "success" and raw_count > 0
                else "failed",
                "collector_status": test_result.get("status"),
                "raw_records_stored": raw_count,
                "test_result": test_result,
                "test_name": "VSIN Collection Test",
            }

            if test_result.get("status") != "success":
                result["error"] = test_result.get("error", "Collection failed")
            elif raw_count == 0:
                result["error"] = "No records stored in raw_data.vsin_data"

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "test_name": "VSIN Collection Test",
            }

    async def _test_raw_data_validation(self) -> dict[str, Any]:
        """Test 2.4: Validate raw data format and completeness."""
        try:
            validation_results = {}

            async with self.db_pool.acquire() as conn:
                # Check Action Network raw data structure
                an_sample = await conn.fetchrow(
                    """
                    SELECT external_game_id, raw_odds, sportsbook_key
                    FROM raw_data.action_network_odds 
                    WHERE created_at >= NOW() - INTERVAL '1 hour'
                    LIMIT 1
                    """
                )

                # Check SBD raw data structure
                sbd_sample = await conn.fetchrow(
                    """
                    SELECT external_matchup_id, raw_response, api_endpoint
                    FROM raw_data.sbd_betting_splits 
                    WHERE created_at >= NOW() - INTERVAL '1 hour'
                    LIMIT 1
                    """
                )

                # Check VSIN raw data structure
                vsin_sample = await conn.fetchrow(
                    """
                    SELECT external_id, data_type, raw_response
                    FROM raw_data.vsin_data 
                    WHERE created_at >= NOW() - INTERVAL '1 hour'
                    LIMIT 1
                    """
                )

            # Validate data structure
            validation_results["action_network"] = {
                "has_sample": an_sample is not None,
                "has_external_id": an_sample
                and an_sample["external_game_id"] is not None,
                "has_raw_data": an_sample and an_sample["raw_odds"] is not None,
                "has_sportsbook": an_sample and an_sample["sportsbook_key"] is not None,
            }

            validation_results["sbd"] = {
                "has_sample": sbd_sample is not None,
                "has_external_id": sbd_sample
                and sbd_sample["external_matchup_id"] is not None,
                "has_raw_response": sbd_sample
                and sbd_sample["raw_response"] is not None,
                "has_api_endpoint": sbd_sample
                and sbd_sample["api_endpoint"] is not None,
            }

            validation_results["vsin"] = {
                "has_sample": vsin_sample is not None,
                "has_external_id": vsin_sample
                and vsin_sample["external_id"] is not None,
                "has_raw_response": vsin_sample
                and vsin_sample["raw_response"] is not None,
                "has_data_type": vsin_sample and vsin_sample["data_type"] is not None,
            }

            # Check if all validations passed
            all_sources_valid = all(
                all(source_validation.values())
                for source_validation in validation_results.values()
            )

            result = {
                "status": "passed" if all_sources_valid else "failed",
                "validation_details": validation_results,
                "test_name": "Raw Data Validation Test",
            }

            if not all_sources_valid:
                failed_sources = [
                    source
                    for source, validations in validation_results.items()
                    if not all(validations.values())
                ]
                result["error"] = (
                    f"Raw data validation failed for sources: {failed_sources}"
                )

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "test_name": "Raw Data Validation Test",
            }

    # ==============================
    # Phase 3: Cross-Source Game ID Resolution Testing
    # ==============================

    async def validate_phase_3_game_resolution(self) -> dict[str, Any]:
        """
        Phase 3: Test cross-source game ID resolution and correlation.

        Tests:
        1. MLB Stats API game ID resolution for current games
        2. Team name normalization across sources
        3. Game matching accuracy between sources
        4. Centralized games table population
        """
        logger.info("Starting Phase 3: Cross-Source Game ID Resolution Testing")
        await self.log_pipeline_execution("phase_3_start", "running")

        phase_results = {
            "phase": "3_game_resolution",
            "status": "running",
            "tests": {},
            "start_time": datetime.now().isoformat(),
        }

        try:
            # Test 3.1: MLB Stats API integration
            mlb_api_results = await self._test_mlb_stats_api_resolution()
            phase_results["tests"]["mlb_stats_api"] = mlb_api_results

            # Test 3.2: Team name normalization
            team_normalization_results = await self._test_team_name_normalization()
            phase_results["tests"]["team_normalization"] = team_normalization_results

            # Test 3.3: Cross-source game matching
            game_matching_results = await self._test_cross_source_game_matching()
            phase_results["tests"]["game_matching"] = game_matching_results

            # Test 3.4: Centralized games table validation
            games_table_results = await self._test_centralized_games_table()
            phase_results["tests"]["centralized_games"] = games_table_results

            # Determine overall phase status
            all_tests_passed = all(
                test["status"] == "passed" for test in phase_results["tests"].values()
            )
            phase_results["status"] = "passed" if all_tests_passed else "failed"
            phase_results["end_time"] = datetime.now().isoformat()

            await self.log_pipeline_execution(
                "phase_3_complete", phase_results["status"], phase_results
            )

            logger.info(
                f"Phase 3 completed: {phase_results['status']}",
                passed_tests=sum(
                    1
                    for t in phase_results["tests"].values()
                    if t["status"] == "passed"
                ),
                total_tests=len(phase_results["tests"]),
            )

            return phase_results

        except Exception as e:
            phase_results["status"] = "error"
            phase_results["error"] = str(e)
            phase_results["end_time"] = datetime.now().isoformat()

            await self.log_pipeline_execution("phase_3_error", "failed", phase_results)
            logger.error(f"Phase 3 failed with error: {str(e)}")

            return phase_results

    async def _test_mlb_stats_api_resolution(self) -> dict[str, Any]:
        """Test 3.1: MLB Stats API game ID resolution."""
        try:
            # Check if we have any games with MLB Stats API game IDs
            async with self.db_pool.acquire() as conn:
                # Check staging.action_network_games for MLB API resolution
                games_with_mlb_id = await conn.fetch(
                    """
                    SELECT external_game_id, mlb_stats_api_game_id, home_team_name, away_team_name
                    FROM staging.action_network_games 
                    WHERE mlb_stats_api_game_id IS NOT NULL
                    AND created_at >= NOW() - INTERVAL '24 hours'
                    LIMIT 5
                    """
                )

                # Also check raw data for MLB API responses
                mlb_api_responses = await conn.fetchval(
                    """
                    SELECT COUNT(*) FROM raw_data.mlb_game_outcomes
                    WHERE created_at >= NOW() - INTERVAL '24 hours'
                    """
                )

            result = {
                "status": "passed"
                if len(games_with_mlb_id) > 0 or mlb_api_responses > 0
                else "warning",
                "games_with_mlb_id": len(games_with_mlb_id),
                "mlb_api_responses": mlb_api_responses,
                "sample_games": [
                    {
                        "external_game_id": game["external_game_id"],
                        "mlb_stats_api_game_id": game["mlb_stats_api_game_id"],
                        "teams": f"{game['away_team_name']} @ {game['home_team_name']}",
                    }
                    for game in games_with_mlb_id[:3]
                ],
                "test_name": "MLB Stats API Resolution Test",
            }

            if len(games_with_mlb_id) == 0 and mlb_api_responses == 0:
                result["warning"] = (
                    "No recent games with MLB Stats API game IDs found - may need MLB API integration"
                )

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "test_name": "MLB Stats API Resolution Test",
            }

    async def _test_team_name_normalization(self) -> dict[str, Any]:
        """Test 3.2: Team name normalization across sources."""
        try:
            async with self.db_pool.acquire() as conn:
                # Get team names from different sources
                action_network_teams = await conn.fetch(
                    """
                    SELECT DISTINCT home_team_name, away_team_name 
                    FROM staging.action_network_games 
                    WHERE created_at >= NOW() - INTERVAL '24 hours'
                    LIMIT 10
                    """
                )

                # Get team names from raw data sources
                sbd_teams = await conn.fetch(
                    """
                    SELECT DISTINCT 
                        raw_response->'game_data'->>'home_team' as home_team,
                        raw_response->'game_data'->>'away_team' as away_team
                    FROM raw_data.sbd_betting_splits 
                    WHERE created_at >= NOW() - INTERVAL '1 hour'
                    LIMIT 10
                    """
                )

                vsin_teams = await conn.fetch(
                    """
                    SELECT DISTINCT 
                        raw_response->>'home_team' as home_team,
                        raw_response->>'away_team' as away_team
                    FROM raw_data.vsin_data 
                    WHERE created_at >= NOW() - INTERVAL '1 hour'
                    LIMIT 10
                    """
                )

            # Analyze team name consistency
            all_teams = set()
            source_teams = {"action_network": set(), "sbd": set(), "vsin": set()}

            # Collect Action Network teams
            for game in action_network_teams:
                if game["home_team_name"]:
                    source_teams["action_network"].add(game["home_team_name"])
                    all_teams.add(game["home_team_name"])
                if game["away_team_name"]:
                    source_teams["action_network"].add(game["away_team_name"])
                    all_teams.add(game["away_team_name"])

            # Collect SBD teams
            for game in sbd_teams:
                if game["home_team"]:
                    source_teams["sbd"].add(game["home_team"])
                    all_teams.add(game["home_team"])
                if game["away_team"]:
                    source_teams["sbd"].add(game["away_team"])
                    all_teams.add(game["away_team"])

            # Collect VSIN teams
            for game in vsin_teams:
                if game["home_team"]:
                    source_teams["vsin"].add(game["home_team"])
                    all_teams.add(game["home_team"])
                if game["away_team"]:
                    source_teams["vsin"].add(game["away_team"])
                    all_teams.add(game["away_team"])

            result = {
                "status": "passed" if len(all_teams) > 0 else "warning",
                "total_unique_teams": len(all_teams),
                "teams_by_source": {
                    source: len(teams) for source, teams in source_teams.items()
                },
                "sample_teams": list(all_teams)[:10],
                "source_overlap": self._calculate_team_overlap(source_teams),
                "test_name": "Team Name Normalization Test",
            }

            if len(all_teams) == 0:
                result["warning"] = (
                    "No team data found across sources - may indicate data collection issues"
                )

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "test_name": "Team Name Normalization Test",
            }

    def _calculate_team_overlap(self, source_teams: dict[str, set]) -> dict[str, Any]:
        """Calculate overlap between team names across sources."""
        overlaps = {}
        sources = list(source_teams.keys())

        for i, source1 in enumerate(sources):
            for source2 in sources[i + 1 :]:
                teams1 = source_teams[source1]
                teams2 = source_teams[source2]

                if teams1 and teams2:
                    intersection = teams1.intersection(teams2)
                    union = teams1.union(teams2)
                    overlap_pct = len(intersection) / len(union) * 100 if union else 0

                    overlaps[f"{source1}_vs_{source2}"] = {
                        "common_teams": len(intersection),
                        "total_unique": len(union),
                        "overlap_percentage": round(overlap_pct, 2),
                    }

        return overlaps

    async def _test_cross_source_game_matching(self) -> dict[str, Any]:
        """Test 3.3: Cross-source game matching accuracy."""
        try:
            async with self.db_pool.acquire() as conn:
                # Find games that appear in multiple sources by date and team matching
                potential_matches = await conn.fetch(
                    """
                    WITH today_games AS (
                        SELECT 
                            'action_network' as source,
                            external_game_id,
                            home_team_name as home_team,
                            away_team_name as away_team,
                            game_date,
                            mlb_stats_api_game_id
                        FROM staging.action_network_games 
                        WHERE game_date = CURRENT_DATE
                    ),
                    sbd_games AS (
                        SELECT DISTINCT
                            'sbd' as source,
                            raw_response->'game_data'->>'external_game_id' as external_game_id,
                            raw_response->'game_data'->>'home_team' as home_team,
                            raw_response->'game_data'->>'away_team' as away_team,
                            CURRENT_DATE as game_date
                        FROM raw_data.sbd_betting_splits 
                        WHERE created_at >= CURRENT_DATE
                    ),
                    vsin_games AS (
                        SELECT DISTINCT
                            'vsin' as source,
                            external_id as external_game_id,
                            raw_response->>'home_team' as home_team,
                            raw_response->>'away_team' as away_team,
                            CURRENT_DATE as game_date
                        FROM raw_data.vsin_data 
                        WHERE created_at >= CURRENT_DATE
                    )
                    SELECT 
                        t.home_team as an_home,
                        t.away_team as an_away,
                        s.home_team as sbd_home,
                        s.away_team as sbd_away,
                        v.home_team as vsin_home,
                        v.away_team as vsin_away,
                        t.mlb_stats_api_game_id
                    FROM today_games t
                    FULL OUTER JOIN sbd_games s ON (
                        UPPER(TRIM(t.home_team)) = UPPER(TRIM(s.home_team)) AND
                        UPPER(TRIM(t.away_team)) = UPPER(TRIM(s.away_team))
                    )
                    FULL OUTER JOIN vsin_games v ON (
                        UPPER(TRIM(COALESCE(t.home_team, s.home_team))) = UPPER(TRIM(v.home_team)) AND
                        UPPER(TRIM(COALESCE(t.away_team, s.away_team))) = UPPER(TRIM(v.away_team))
                    )
                    WHERE t.home_team IS NOT NULL OR s.home_team IS NOT NULL OR v.home_team IS NOT NULL
                    LIMIT 15
                    """
                )

            # Analyze matching quality
            matches_found = len(potential_matches)
            perfect_matches = 0
            partial_matches = 0

            for match in potential_matches:
                sources_present = sum(
                    [
                        1 if match["an_home"] else 0,
                        1 if match["sbd_home"] else 0,
                        1 if match["vsin_home"] else 0,
                    ]
                )

                if sources_present >= 3:
                    perfect_matches += 1
                elif sources_present >= 2:
                    partial_matches += 1

            result = {
                "status": "passed" if matches_found > 0 else "warning",
                "total_game_matches": matches_found,
                "perfect_matches": perfect_matches,  # All 3 sources
                "partial_matches": partial_matches,  # 2 sources
                "match_rate": {
                    "perfect": round(perfect_matches / matches_found * 100, 2)
                    if matches_found > 0
                    else 0,
                    "partial": round(partial_matches / matches_found * 100, 2)
                    if matches_found > 0
                    else 0,
                },
                "sample_matches": [
                    {
                        "action_network": f"{match['an_away']} @ {match['an_home']}"
                        if match["an_home"]
                        else None,
                        "sbd": f"{match['sbd_away']} @ {match['sbd_home']}"
                        if match["sbd_home"]
                        else None,
                        "vsin": f"{match['vsin_away']} @ {match['vsin_home']}"
                        if match["vsin_home"]
                        else None,
                        "mlb_api_id": match["mlb_stats_api_game_id"],
                    }
                    for match in potential_matches[:5]
                ],
                "test_name": "Cross-Source Game Matching Test",
            }

            if matches_found == 0:
                result["warning"] = (
                    "No cross-source game matches found - may indicate team name normalization issues"
                )

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "test_name": "Cross-Source Game Matching Test",
            }

    async def _test_centralized_games_table(self) -> dict[str, Any]:
        """Test 3.4: Centralized games table validation."""
        try:
            async with self.db_pool.acquire() as conn:
                # Check staging.action_network_games as the centralized games table
                games_stats = await conn.fetchrow(
                    """
                    SELECT 
                        COUNT(*) as total_games,
                        COUNT(DISTINCT external_game_id) as unique_games,
                        COUNT(mlb_stats_api_game_id) as games_with_mlb_id,
                        COUNT(DISTINCT game_date) as unique_dates,
                        MIN(game_date) as earliest_game,
                        MAX(game_date) as latest_game,
                        COUNT(*) FILTER (WHERE data_quality_score >= 0.8) as high_quality_games
                    FROM staging.action_network_games
                    WHERE created_at >= NOW() - INTERVAL '7 days'
                    """
                )

                # Check for recent game updates
                recent_games = await conn.fetchval(
                    """
                    SELECT COUNT(*) FROM staging.action_network_games 
                    WHERE updated_at >= NOW() - INTERVAL '1 hour'
                    """
                )

                # Validate game data completeness
                data_completeness = await conn.fetchrow(
                    """
                    SELECT 
                        COUNT(*) FILTER (WHERE home_team_name IS NOT NULL AND away_team_name IS NOT NULL) as complete_teams,
                        COUNT(*) FILTER (WHERE game_date IS NOT NULL) as complete_dates,
                        COUNT(*) FILTER (WHERE game_status IS NOT NULL) as complete_status,
                        COUNT(*) as total_games
                    FROM staging.action_network_games
                    WHERE created_at >= NOW() - INTERVAL '24 hours'
                    """
                )

            total_games = games_stats["total_games"] or 0
            completeness_pct = {
                "teams": round(
                    (
                        data_completeness["complete_teams"]
                        / data_completeness["total_games"]
                        * 100
                    ),
                    2,
                )
                if data_completeness["total_games"] > 0
                else 0,
                "dates": round(
                    (
                        data_completeness["complete_dates"]
                        / data_completeness["total_games"]
                        * 100
                    ),
                    2,
                )
                if data_completeness["total_games"] > 0
                else 0,
                "status": round(
                    (
                        data_completeness["complete_status"]
                        / data_completeness["total_games"]
                        * 100
                    ),
                    2,
                )
                if data_completeness["total_games"] > 0
                else 0,
            }

            result = {
                "status": "passed"
                if total_games > 0 and completeness_pct["teams"] >= 90
                else "warning",
                "total_games": total_games,
                "unique_games": games_stats["unique_games"],
                "games_with_mlb_id": games_stats["games_with_mlb_id"],
                "mlb_id_coverage": round(
                    (games_stats["games_with_mlb_id"] / total_games * 100), 2
                )
                if total_games > 0
                else 0,
                "recent_updates": recent_games,
                "date_range": {
                    "earliest": games_stats["earliest_game"].isoformat()
                    if games_stats["earliest_game"]
                    else None,
                    "latest": games_stats["latest_game"].isoformat()
                    if games_stats["latest_game"]
                    else None,
                    "unique_dates": games_stats["unique_dates"],
                },
                "data_completeness": completeness_pct,
                "high_quality_games": games_stats["high_quality_games"],
                "quality_rate": round(
                    (games_stats["high_quality_games"] / total_games * 100), 2
                )
                if total_games > 0
                else 0,
                "test_name": "Centralized Games Table Test",
            }

            if total_games == 0:
                result["error"] = "No games found in centralized games table"
            elif completeness_pct["teams"] < 90:
                result["warning"] = (
                    f"Low team data completeness: {completeness_pct['teams']}%"
                )

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "test_name": "Centralized Games Table Test",
            }

    # ==============================
    # Phase 4: Pipeline Processing Validation Tests
    # ==============================

    async def validate_phase_4_pipeline_processing(self) -> dict[str, Any]:
        """
        Phase 4: Test data processing through RAW â†’ STAGING â†’ CURATED pipeline.

        Tests:
        1. Raw data to staging data processing
        2. Data transformation and normalization
        3. Quality scoring and validation workflows
        4. Pipeline execution tracking and logging
        """
        logger.info("Starting Phase 4: Pipeline Processing Validation Testing")
        await self.log_pipeline_execution("phase_4_start", "running")

        phase_results = {
            "phase": "4_pipeline_processing",
            "status": "running",
            "tests": {},
            "start_time": datetime.now().isoformat(),
        }

        try:
            # Test 4.1: Raw to staging processing
            raw_staging_results = await self._test_raw_to_staging_processing()
            phase_results["tests"]["raw_to_staging"] = raw_staging_results

            # Test 4.2: Data transformation validation
            transformation_results = await self._test_data_transformation()
            phase_results["tests"]["data_transformation"] = transformation_results

            # Test 4.3: Quality scoring workflow
            quality_results = await self._test_quality_scoring_workflow()
            phase_results["tests"]["quality_scoring"] = quality_results

            # Test 4.4: Pipeline execution tracking
            execution_tracking_results = await self._test_pipeline_execution_tracking()
            phase_results["tests"]["execution_tracking"] = execution_tracking_results

            # Determine overall phase status
            all_tests_passed = all(
                test["status"] == "passed" for test in phase_results["tests"].values()
            )
            phase_results["status"] = "passed" if all_tests_passed else "failed"
            phase_results["end_time"] = datetime.now().isoformat()

            await self.log_pipeline_execution(
                "phase_4_complete", phase_results["status"], phase_results
            )

            logger.info(
                f"Phase 4 completed: {phase_results['status']}",
                passed_tests=sum(
                    1
                    for t in phase_results["tests"].values()
                    if t["status"] == "passed"
                ),
                total_tests=len(phase_results["tests"]),
            )

            return phase_results

        except Exception as e:
            phase_results["status"] = "error"
            phase_results["error"] = str(e)
            phase_results["end_time"] = datetime.now().isoformat()

            await self.log_pipeline_execution("phase_4_error", "failed", phase_results)
            logger.error(f"Phase 4 failed with error: {str(e)}")

            return phase_results

    async def _test_raw_to_staging_processing(self) -> dict[str, Any]:
        """Test 4.1: Raw data to staging data processing."""
        try:
            async with self.db_pool.acquire() as conn:
                # Check raw data processing into staging
                processing_stats = await conn.fetchrow(
                    """
                    WITH raw_counts AS (
                        SELECT 
                            COUNT(*) as action_network_raw,
                            COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '1 hour') as recent_raw
                        FROM raw_data.action_network_odds
                    ),
                    staging_counts AS (
                        SELECT 
                            COUNT(*) as action_network_staging,
                            COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '1 hour') as recent_staging,
                            COUNT(*) FILTER (WHERE raw_data_id IS NOT NULL) as with_raw_references
                        FROM staging.action_network_games
                    )
                    SELECT 
                        r.action_network_raw,
                        r.recent_raw,
                        s.action_network_staging,
                        s.recent_staging,
                        s.with_raw_references
                    FROM raw_counts r, staging_counts s
                    """
                )

                # Check historical odds processing
                historical_odds_stats = await conn.fetchrow(
                    """
                    SELECT 
                        COUNT(*) as total_historical_odds,
                        COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '1 hour') as recent_historical,
                        COUNT(DISTINCT external_game_id) as unique_games,
                        COUNT(DISTINCT sportsbook_external_id) as unique_sportsbooks,
                        AVG(data_quality_score) as avg_quality_score
                    FROM staging.action_network_odds_historical
                    """
                )

            # Calculate processing efficiency
            raw_total = processing_stats["action_network_raw"] or 0
            staging_total = processing_stats["action_network_staging"] or 0
            processing_rate = (staging_total / raw_total * 100) if raw_total > 0 else 0

            result = {
                "status": "passed"
                if staging_total > 0 and processing_rate > 50
                else "warning",
                "raw_data_records": raw_total,
                "staging_records": staging_total,
                "recent_raw": processing_stats["recent_raw"],
                "recent_staging": processing_stats["recent_staging"],
                "processing_rate": round(processing_rate, 2),
                "raw_references": processing_stats["with_raw_references"],
                "historical_odds": {
                    "total": historical_odds_stats["total_historical_odds"],
                    "recent": historical_odds_stats["recent_historical"],
                    "unique_games": historical_odds_stats["unique_games"],
                    "unique_sportsbooks": historical_odds_stats["unique_sportsbooks"],
                    "avg_quality": round(
                        float(historical_odds_stats["avg_quality_score"]), 2
                    )
                    if historical_odds_stats["avg_quality_score"]
                    else 0,
                },
                "test_name": "Raw to Staging Processing Test",
            }

            if staging_total == 0:
                result["error"] = (
                    "No staging data found - processing pipeline may not be working"
                )
            elif processing_rate < 50:
                result["warning"] = (
                    f"Low processing rate: {processing_rate}% - may indicate processing issues"
                )

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "test_name": "Raw to Staging Processing Test",
            }

    async def _test_data_transformation(self) -> dict[str, Any]:
        """Test 4.2: Data transformation and normalization validation."""
        try:
            async with self.db_pool.acquire() as conn:
                # Check data transformation quality in staging
                transformation_stats = await conn.fetchrow(
                    """
                    SELECT 
                        COUNT(*) as total_games,
                        COUNT(*) FILTER (WHERE home_team_normalized IS NOT NULL AND away_team_normalized IS NOT NULL) as normalized_teams,
                        COUNT(*) FILTER (WHERE data_quality_score >= 0.8) as high_quality,
                        COUNT(*) FILTER (WHERE validation_status = 'valid') as validated_records,
                        COUNT(*) FILTER (WHERE validation_errors IS NOT NULL AND array_length(validation_errors, 1) > 0) as with_errors,
                        COUNT(*) FILTER (WHERE validation_warnings IS NOT NULL AND array_length(validation_warnings, 1) > 0) as with_warnings,
                        AVG(data_quality_score) as avg_quality_score
                    FROM staging.action_network_games
                    WHERE created_at >= NOW() - INTERVAL '24 hours'
                    """
                )

                # Check odds normalization in historical table
                odds_normalization = await conn.fetchrow(
                    """
                    SELECT 
                        COUNT(*) as total_odds,
                        COUNT(*) FILTER (WHERE odds IS NOT NULL) as with_odds,
                        COUNT(*) FILTER (WHERE market_type IN ('moneyline', 'spread', 'total')) as valid_market_types,
                        COUNT(*) FILTER (WHERE side IN ('home', 'away', 'over', 'under')) as valid_sides,
                        COUNT(DISTINCT market_type) as unique_market_types,
                        COUNT(DISTINCT side) as unique_sides
                    FROM staging.action_network_odds_historical
                    WHERE created_at >= NOW() - INTERVAL '1 hour'
                    """
                )

            total_games = transformation_stats["total_games"] or 0

            # Calculate transformation quality metrics
            transformation_metrics = {
                "team_normalization_rate": round(
                    (transformation_stats["normalized_teams"] / total_games * 100), 2
                )
                if total_games > 0
                else 0,
                "validation_rate": round(
                    (transformation_stats["validated_records"] / total_games * 100), 2
                )
                if total_games > 0
                else 0,
                "high_quality_rate": round(
                    (transformation_stats["high_quality"] / total_games * 100), 2
                )
                if total_games > 0
                else 0,
                "error_rate": round(
                    (transformation_stats["with_errors"] / total_games * 100), 2
                )
                if total_games > 0
                else 0,
            }

            result = {
                "status": "passed"
                if transformation_metrics["team_normalization_rate"] >= 90
                and transformation_metrics["validation_rate"] >= 80
                else "warning",
                "total_games": total_games,
                "transformation_metrics": transformation_metrics,
                "avg_quality_score": round(
                    float(transformation_stats["avg_quality_score"]), 2
                )
                if transformation_stats["avg_quality_score"]
                else 0,
                "validation_summary": {
                    "validated": transformation_stats["validated_records"],
                    "with_errors": transformation_stats["with_errors"],
                    "with_warnings": transformation_stats["with_warnings"],
                },
                "odds_normalization": {
                    "total_odds": odds_normalization["total_odds"],
                    "with_odds": odds_normalization["with_odds"],
                    "valid_market_types": odds_normalization["valid_market_types"],
                    "valid_sides": odds_normalization["valid_sides"],
                    "unique_markets": odds_normalization["unique_market_types"],
                    "unique_sides": odds_normalization["unique_sides"],
                },
                "test_name": "Data Transformation Test",
            }

            if transformation_metrics["team_normalization_rate"] < 90:
                result["warning"] = (
                    f"Low team normalization rate: {transformation_metrics['team_normalization_rate']}%"
                )

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "test_name": "Data Transformation Test",
            }

    async def _test_quality_scoring_workflow(self) -> dict[str, Any]:
        """Test 4.3: Quality scoring and validation workflow."""
        try:
            async with self.db_pool.acquire() as conn:
                # Check quality scoring distribution
                quality_distribution = await conn.fetch(
                    """
                    SELECT 
                        CASE 
                            WHEN data_quality_score >= 0.9 THEN 'excellent'
                            WHEN data_quality_score >= 0.8 THEN 'good'
                            WHEN data_quality_score >= 0.7 THEN 'fair'
                            ELSE 'poor'
                        END as quality_tier,
                        COUNT(*) as record_count
                    FROM staging.action_network_games
                    WHERE created_at >= NOW() - INTERVAL '24 hours'
                    GROUP BY quality_tier
                    ORDER BY quality_tier
                    """
                )

                # Check validation status distribution
                validation_distribution = await conn.fetch(
                    """
                    SELECT 
                        validation_status,
                        COUNT(*) as status_count
                    FROM staging.action_network_games
                    WHERE created_at >= NOW() - INTERVAL '24 hours'
                    GROUP BY validation_status
                    ORDER BY status_count DESC
                    """
                )

                # Check data quality metrics table if it exists
                quality_metrics = await conn.fetchrow(
                    """
                    SELECT 
                        COUNT(*) as total_metrics,
                        COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '1 hour') as recent_metrics
                    FROM staging.data_quality_metrics
                    WHERE created_at >= NOW() - INTERVAL '24 hours'
                    """
                )

            # Process quality distribution
            quality_summary = {}
            total_records = 0
            for tier in quality_distribution:
                quality_summary[tier["quality_tier"]] = tier["record_count"]
                total_records += tier["record_count"]

            validation_summary = {}
            for status in validation_distribution:
                validation_summary[status["validation_status"]] = status["status_count"]

            # Calculate quality health metrics
            excellent_rate = (
                (quality_summary.get("excellent", 0) / total_records * 100)
                if total_records > 0
                else 0
            )
            good_plus_rate = (
                (
                    (
                        quality_summary.get("excellent", 0)
                        + quality_summary.get("good", 0)
                    )
                    / total_records
                    * 100
                )
                if total_records > 0
                else 0
            )

            result = {
                "status": "passed" if good_plus_rate >= 80 else "warning",
                "total_records": total_records,
                "quality_distribution": quality_summary,
                "validation_distribution": validation_summary,
                "quality_health": {
                    "excellent_rate": round(excellent_rate, 2),
                    "good_plus_rate": round(good_plus_rate, 2),
                    "quality_metrics_tracked": quality_metrics["total_metrics"],
                    "recent_metrics": quality_metrics["recent_metrics"],
                },
                "test_name": "Quality Scoring Workflow Test",
            }

            if good_plus_rate < 80:
                result["warning"] = f"Low quality rate: {good_plus_rate}% good+ records"

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "test_name": "Quality Scoring Workflow Test",
            }

    async def _test_pipeline_execution_tracking(self) -> dict[str, Any]:
        """Test 4.4: Pipeline execution tracking and logging."""
        try:
            async with self.db_pool.acquire() as conn:
                # Check pipeline execution logs
                execution_stats = await conn.fetchrow(
                    """
                    SELECT 
                        COUNT(*) as total_executions,
                        COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '1 hour') as recent_executions,
                        COUNT(*) FILTER (WHERE status = 'completed') as successful_executions,
                        COUNT(*) FILTER (WHERE status = 'failed') as failed_executions,
                        COUNT(*) FILTER (WHERE status = 'running') as running_executions,
                        COUNT(DISTINCT pipeline_stage) as unique_stages,
                        COUNT(DISTINCT execution_id) as unique_execution_ids
                    FROM public.pipeline_execution_log
                    WHERE created_at >= NOW() - INTERVAL '24 hours'
                    """
                )

                # Get recent pipeline stages
                recent_stages = await conn.fetch(
                    """
                    SELECT 
                        pipeline_stage,
                        COUNT(*) as execution_count,
                        MAX(created_at) as last_execution
                    FROM public.pipeline_execution_log
                    WHERE created_at >= NOW() - INTERVAL '6 hours'
                    GROUP BY pipeline_stage
                    ORDER BY last_execution DESC
                    LIMIT 10
                    """
                )

                # Check for any long-running executions
                long_running = await conn.fetchval(
                    """
                    SELECT COUNT(*) 
                    FROM public.pipeline_execution_log
                    WHERE status = 'running' 
                    AND start_time < NOW() - INTERVAL '1 hour'
                    """
                )

            total_executions = execution_stats["total_executions"] or 0
            success_rate = (
                (execution_stats["successful_executions"] / total_executions * 100)
                if total_executions > 0
                else 0
            )

            stages_summary = {}
            for stage in recent_stages:
                stages_summary[stage["pipeline_stage"]] = {
                    "execution_count": stage["execution_count"],
                    "last_execution": stage["last_execution"].isoformat()
                    if stage["last_execution"]
                    else None,
                }

            result = {
                "status": "passed"
                if total_executions > 0 and success_rate >= 80
                else "warning",
                "execution_summary": {
                    "total_executions": total_executions,
                    "recent_executions": execution_stats["recent_executions"],
                    "successful": execution_stats["successful_executions"],
                    "failed": execution_stats["failed_executions"],
                    "running": execution_stats["running_executions"],
                    "success_rate": round(success_rate, 2),
                },
                "pipeline_stages": {
                    "unique_stages": execution_stats["unique_stages"],
                    "recent_stages": stages_summary,
                },
                "execution_health": {
                    "unique_execution_ids": execution_stats["unique_execution_ids"],
                    "long_running_executions": long_running,
                },
                "test_name": "Pipeline Execution Tracking Test",
            }

            if total_executions == 0:
                result["warning"] = (
                    "No pipeline executions found - tracking may not be working"
                )
            elif success_rate < 80:
                result["warning"] = f"Low success rate: {success_rate}%"
            elif long_running > 0:
                result["warning"] = f"{long_running} long-running executions detected"

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "test_name": "Pipeline Execution Tracking Test",
            }

    # ==============================
    # Utility Methods
    # ==============================

    async def run_full_validation(self) -> dict[str, Any]:
        """Run complete validation of all phases."""
        logger.info("Starting full three-tier pipeline validation")

        full_results = {
            "execution_id": self.execution_id,
            "start_time": datetime.now().isoformat(),
            "phases": {},
            "overall_status": "running",
        }

        try:
            # Phase 1: Schema validation
            phase_1_results = await self.validate_phase_1_schemas()
            full_results["phases"]["phase_1"] = phase_1_results

            # Only proceed to Phase 2 if Phase 1 passed
            if phase_1_results["status"] == "passed":
                phase_2_results = await self.validate_phase_2_sources()
                full_results["phases"]["phase_2"] = phase_2_results

                # Only proceed to Phase 3 if Phase 2 has sufficient data
                if phase_2_results["status"] == "passed":
                    phase_3_results = await self.validate_phase_3_game_resolution()
                    full_results["phases"]["phase_3"] = phase_3_results

                    # Only proceed to Phase 4 if Phase 3 has adequate game resolution
                    if phase_3_results["status"] in ["passed", "warning"]:
                        phase_4_results = (
                            await self.validate_phase_4_pipeline_processing()
                        )
                        full_results["phases"]["phase_4"] = phase_4_results
                    else:
                        logger.warning("Skipping Phase 4 due to Phase 3 issues")
                        full_results["phases"]["phase_4"] = {
                            "status": "skipped",
                            "reason": "Phase 3 validation issues",
                        }
                else:
                    logger.warning("Skipping Phase 3 due to Phase 2 issues")
                    full_results["phases"]["phase_3"] = {
                        "status": "skipped",
                        "reason": "Phase 2 validation issues",
                    }
                    full_results["phases"]["phase_4"] = {
                        "status": "skipped",
                        "reason": "Phase 2 validation issues",
                    }
            else:
                logger.warning("Skipping Phase 2 due to Phase 1 failures")
                full_results["phases"]["phase_2"] = {
                    "status": "skipped",
                    "reason": "Phase 1 validation failed",
                }
                full_results["phases"]["phase_3"] = {
                    "status": "skipped",
                    "reason": "Phase 1 validation failed",
                }
                full_results["phases"]["phase_4"] = {
                    "status": "skipped",
                    "reason": "Phase 1 validation failed",
                }

            # Determine overall status
            completed_phases = [
                phase
                for phase in full_results["phases"].values()
                if phase["status"] != "skipped"
            ]

            if not completed_phases:
                full_results["overall_status"] = "failed"
            elif all(phase["status"] == "passed" for phase in completed_phases):
                full_results["overall_status"] = "passed"
            else:
                full_results["overall_status"] = "failed"

            full_results["end_time"] = datetime.now().isoformat()

            # Log final results
            await self.log_pipeline_execution(
                "full_validation_complete", full_results["overall_status"], full_results
            )

            logger.info(
                f"Full validation completed: {full_results['overall_status']}",
                execution_id=self.execution_id,
            )

            return full_results

        except Exception as e:
            full_results["overall_status"] = "error"
            full_results["error"] = str(e)
            full_results["end_time"] = datetime.now().isoformat()

            await self.log_pipeline_execution(
                "full_validation_error", "failed", full_results
            )

            logger.error(f"Full validation failed: {str(e)}")
            return full_results


# ==============================
# Pytest Integration
# ==============================


@pytest.mark.asyncio
async def test_phase_1_schema_validation():
    """Test Phase 1: Database schema validation."""
    async with ThreeTierPipelineValidator() as validator:
        results = await validator.validate_phase_1_schemas()

        assert results["status"] in ["passed", "failed"], (
            f"Invalid status: {results['status']}"
        )

        if results["status"] == "failed":
            pytest.fail(
                f"Phase 1 validation failed: {results.get('error', 'Unknown error')}"
            )


@pytest.mark.asyncio
async def test_phase_2_source_validation():
    """Test Phase 2: Individual source validation."""
    async with ThreeTierPipelineValidator() as validator:
        # First ensure Phase 1 passes
        phase_1_results = await validator.validate_phase_1_schemas()
        if phase_1_results["status"] != "passed":
            pytest.skip("Phase 1 validation failed, skipping Phase 2")

        results = await validator.validate_phase_2_sources()

        assert results["status"] in ["passed", "failed"], (
            f"Invalid status: {results['status']}"
        )

        if results["status"] == "failed":
            pytest.fail(
                f"Phase 2 validation failed: {results.get('error', 'Unknown error')}"
            )


@pytest.mark.asyncio
async def test_phase_3_game_resolution():
    """Test Phase 3: Cross-source game ID resolution."""
    async with ThreeTierPipelineValidator() as validator:
        # Ensure Phase 1 and 2 are working first
        phase_1_results = await validator.validate_phase_1_schemas()
        if phase_1_results["status"] != "passed":
            pytest.skip("Phase 1 validation failed, skipping Phase 3")

        phase_2_results = await validator.validate_phase_2_sources()
        if phase_2_results["status"] != "passed":
            pytest.skip("Phase 2 validation failed, skipping Phase 3")

        results = await validator.validate_phase_3_game_resolution()

        assert results["status"] in ["passed", "failed"], (
            f"Invalid status: {results['status']}"
        )

        if results["status"] == "failed":
            pytest.fail(
                f"Phase 3 validation failed: {results.get('error', 'Unknown error')}"
            )


@pytest.mark.asyncio
async def test_phase_4_pipeline_processing():
    """Test Phase 4: Pipeline processing validation."""
    async with ThreeTierPipelineValidator() as validator:
        # This phase can run independently for pipeline processing validation
        results = await validator.validate_phase_4_pipeline_processing()

        assert results["status"] in ["passed", "failed", "warning"], (
            f"Invalid status: {results['status']}"
        )

        # Phase 4 can have warnings (e.g., low processing rates) but still be functional
        if results["status"] == "failed":
            pytest.fail(
                f"Phase 4 validation failed: {results.get('error', 'Unknown error')}"
            )


@pytest.mark.asyncio
async def test_full_pipeline_validation():
    """Test complete pipeline validation."""
    async with ThreeTierPipelineValidator() as validator:
        results = await validator.run_full_validation()

        assert results["overall_status"] in ["passed", "failed"], (
            f"Invalid status: {results['overall_status']}"
        )

        # Log results for debugging
        logger.info("Full validation results", results=results)

        if results["overall_status"] == "failed":
            pytest.fail(
                f"Full pipeline validation failed: {results.get('error', 'See phase details')}"
            )


@pytest.mark.asyncio
async def test_end_to_end_pipeline_framework():
    """
    Comprehensive end-to-end pipeline testing framework.

    This test validates the entire three-tier pipeline from data collection
    through processing to final curated data, with detailed diagnostics.
    """
    async with ThreeTierPipelineValidator() as validator:
        # Run comprehensive validation with detailed reporting
        full_results = await validator.run_full_validation()

        # Enhanced reporting for end-to-end validation
        end_to_end_summary = {
            "framework_status": "comprehensive_testing",
            "validation_execution_id": full_results["execution_id"],
            "total_phases": len(full_results["phases"]),
            "completed_phases": sum(
                1 for p in full_results["phases"].values() if p["status"] != "skipped"
            ),
            "passed_phases": sum(
                1 for p in full_results["phases"].values() if p["status"] == "passed"
            ),
            "failed_phases": sum(
                1 for p in full_results["phases"].values() if p["status"] == "failed"
            ),
            "warning_phases": sum(
                1 for p in full_results["phases"].values() if p["status"] == "warning"
            ),
            "skipped_phases": sum(
                1 for p in full_results["phases"].values() if p["status"] == "skipped"
            ),
            "overall_health": full_results["overall_status"],
            "pipeline_readiness": _determine_pipeline_readiness(full_results),
            "critical_issues": _extract_critical_issues(full_results),
            "recommendations": _generate_recommendations(full_results),
            "data_flow_status": _assess_data_flow(full_results),
            "timestamp": datetime.now().isoformat(),
        }

        # Log comprehensive summary
        logger.info(
            "End-to-end pipeline framework validation completed",
            summary=end_to_end_summary,
        )

        # Validate framework completeness
        assert full_results["overall_status"] in ["passed", "failed"], (
            f"Invalid overall status: {full_results['overall_status']}"
        )

        # Check that all expected phases were considered
        expected_phases = {"phase_1", "phase_2", "phase_3", "phase_4"}
        actual_phases = set(full_results["phases"].keys())
        assert expected_phases.issubset(actual_phases), (
            f"Missing phases: {expected_phases - actual_phases}"
        )

        # Framework should be considered successful if pipeline architecture is sound
        # Phase 1 failures due to legacy schema are acceptable for development
        # Phase 4 can run independently and provides critical pipeline insights

        framework_success = (
            # Either Phase 1 passes OR Phase 4 provides meaningful pipeline validation
            full_results["phases"]["phase_1"]["status"] == "passed"
            or full_results["phases"]["phase_4"]["status"] in ["passed", "warning"]
            or
            # Or we have sufficient phases working to demonstrate pipeline health
            sum(
                1
                for p in full_results["phases"].values()
                if p["status"] in ["passed", "warning"]
            )
            >= 2
        )

        # Log the framework decision criteria
        logger.info(
            "Framework success evaluation",
            framework_success=framework_success,
            passed_warning_phases=sum(
                1
                for p in full_results["phases"].values()
                if p["status"] in ["passed", "warning"]
            ),
            pipeline_readiness=end_to_end_summary["pipeline_readiness"],
        )

        if not framework_success:
            pytest.fail(
                "End-to-end framework validation failed: Insufficient pipeline validation"
            )

        return end_to_end_summary


def _determine_pipeline_readiness(results: dict[str, Any]) -> str:
    """Determine overall pipeline readiness based on validation results."""
    phase_statuses = [phase["status"] for phase in results["phases"].values()]

    if "failed" not in phase_statuses and results["overall_status"] == "passed":
        return "production_ready"
    elif sum(1 for s in phase_statuses if s in ["passed", "warning"]) >= 3:
        return "staging_ready"
    elif sum(1 for s in phase_statuses if s in ["passed", "warning"]) >= 2:
        return "development_ready"
    else:
        return "not_ready"


def _extract_critical_issues(results: dict[str, Any]) -> list[str]:
    """Extract critical issues from validation results."""
    issues = []

    for phase_name, phase_data in results["phases"].items():
        if phase_data["status"] == "failed":
            if "error" in phase_data:
                issues.append(f"{phase_name}: {phase_data['error']}")
            elif "tests" in phase_data:
                failed_tests = [
                    test
                    for test, data in phase_data["tests"].items()
                    if data.get("status") == "failed"
                ]
                if failed_tests:
                    issues.append(
                        f"{phase_name}: Failed tests - {', '.join(failed_tests)}"
                    )

    return issues


def _generate_recommendations(results: dict[str, Any]) -> list[str]:
    """Generate actionable recommendations based on validation results."""
    recommendations = []

    # Check Phase 1 issues
    if results["phases"].get("phase_1", {}).get("status") == "failed":
        phase_1_tests = results["phases"]["phase_1"].get("tests", {})
        if phase_1_tests.get("staging_tables", {}).get("status") == "failed":
            recommendations.append(
                "Migrate legacy core_betting schema tables to three-tier architecture"
            )

    # Check Phase 4 issues
    if results["phases"].get("phase_4", {}).get("status") in ["failed", "warning"]:
        phase_4_tests = results["phases"]["phase_4"].get("tests", {})
        if phase_4_tests.get("raw_to_staging", {}).get("processing_rate", 0) < 10:
            recommendations.append(
                "Investigate low RAW to STAGING processing rate - possible pipeline bottleneck"
            )
        if (
            phase_4_tests.get("execution_tracking", {})
            .get("execution_summary", {})
            .get("success_rate", 0)
            < 50
        ):
            recommendations.append(
                "Improve pipeline execution success rate - review error handling"
            )

    # General recommendations
    if not recommendations:
        recommendations.append(
            "Pipeline validation successful - monitor ongoing data quality metrics"
        )

    return recommendations


def _assess_data_flow(results: dict[str, Any]) -> str:
    """Assess data flow health across the pipeline."""
    phase_2 = results["phases"].get("phase_2", {})
    phase_4 = results["phases"].get("phase_4", {})

    if phase_2.get("status") == "passed" and phase_4.get("status") in [
        "passed",
        "warning",
    ]:
        return "healthy"
    elif phase_2.get("status") in ["passed", "warning"]:
        return "partial"
    else:
        return "impaired"


# ==============================
# CLI Integration
# ==============================


async def main():
    """CLI entry point for running validation."""
    import sys

    if len(sys.argv) > 1:
        phase = sys.argv[1].lower()
    else:
        phase = "full"

    async with ThreeTierPipelineValidator() as validator:
        if phase == "1" or phase == "phase1":
            results = await validator.validate_phase_1_schemas()
        elif phase == "2" or phase == "phase2":
            results = await validator.validate_phase_2_sources()
        elif phase == "3" or phase == "phase3":
            results = await validator.validate_phase_3_game_resolution()
        elif phase == "4" or phase == "phase4":
            results = await validator.validate_phase_4_pipeline_processing()
        else:
            results = await validator.run_full_validation()

        print(json.dumps(results, indent=2, default=str))

        # Exit with appropriate code
        if (
            results.get("overall_status") == "passed"
            or results.get("status") == "passed"
        ):
            sys.exit(0)
        else:
            sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
