[database]
# Database file path
# path = "data/raw/mlb_betting.duckdb"  # Migrated to PostgreSQL

[schemas]
# Pipeline zone schemas
raw = "raw_data"
staging = "staging"
curated = "curated"
# Legacy schema
legacy = "splits"

[pipeline]
# Pipeline configuration
enable_staging = true
enable_curated = true  # Enabled for full pipeline execution
auto_promotion = true  # Re-enabled with zone-specific checks
validation_enabled = true
quality_threshold = 0.8  # Minimum quality score for promotion

[pipeline.zones]
raw_enabled = true
staging_enabled = true
curated_enabled = true  # Enabled for full pipeline execution

[tables]
# Main table for MLB betting splits
mlb_betting_splits = "raw_mlb_betting_splits"

# Legacy table (for backward compatibility)
legacy_splits = "splits"

[data_sources]
# Source identifiers
sbd = "SBD"  # SportsBettingDime
vsin = "VSIN"  # VSIN

[api]
# SportsBettingDime API configuration
sbd_url = "https://srfeeds.sportsbettingdime.com/v2/matchups/mlb/betting-splits"
sbd_books = ["betmgm", "bet365", "fanatics", "draftkings", "caesars", "fanduel"]

[monitoring]
# Prometheus metrics configuration
enable_metrics_sampling = true
metrics_sample_rate = 0.1  # 10% sampling for high-volume operations
max_unmapped_resolution_limit = 100  # Maximum unmapped IDs to resolve per batch

# Metric bucket configurations (in seconds)
pipeline_duration_buckets = [1.0, 5.0, 10.0, 30.0, 60.0, 120.0, 300.0, 600.0]
pipeline_stage_duration_buckets = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]
database_query_duration_buckets = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
api_call_duration_buckets = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]

[ml_pipeline]
# ML Pipeline Configuration

# Feature Pipeline Settings
feature_cache_ttl_seconds = 900  # 15 minutes for feature caching
batch_processing_max_size = 50   # Maximum batch size for feature processing
batch_processing_min_size = 5    # Minimum batch size for feature processing
max_concurrent_extractions = 5   # Maximum concurrent feature extractions

# Memory Management
memory_threshold_mb = 2048       # Memory threshold in MB before triggering cleanup
memory_cleanup_trigger_mb = 500  # Memory increase threshold to trigger cleanup

# Model Loading
model_loading_timeout_seconds = 30  # Timeout for model loading operations
model_cache_size = 10              # Maximum number of models to keep in memory

# Redis Feature Store
redis_socket_timeout = 5.0         # Redis socket timeout in seconds
redis_connection_pool_size = 20    # Redis connection pool size
redis_max_retries = 3              # Maximum Redis connection retries
redis_retry_delay_seconds = 1.0    # Initial retry delay in seconds

# Prediction Service
prediction_batch_size = 10         # Batch size for prediction processing
prediction_cache_ttl_hours = 4     # Cache TTL for predictions in hours

# Performance Targets
api_response_target_ms = 100       # Target API response time in milliseconds  
prediction_latency_target_ms = 500 # Target prediction latency in milliseconds

# Resource Monitoring Thresholds
cpu_warning_threshold = 70.0       # CPU usage warning threshold percentage
cpu_critical_threshold = 85.0      # CPU usage critical threshold percentage
cpu_emergency_threshold = 95.0     # CPU usage emergency threshold percentage

memory_warning_threshold = 75.0    # Memory usage warning threshold percentage
memory_critical_threshold = 85.0   # Memory usage critical threshold percentage
memory_emergency_threshold = 95.0  # Memory usage emergency threshold percentage

disk_warning_threshold = 80.0      # Disk usage warning threshold percentage
disk_critical_threshold = 90.0     # Disk usage critical threshold percentage
disk_emergency_threshold = 95.0    # Disk usage emergency threshold percentage

resource_monitoring_interval = 10  # Resource monitoring interval in seconds
resource_alert_cooldown = 300      # Resource alert cooldown period in seconds

# =============================================================================
# ML SYSTEM CONFIGURATION
# =============================================================================

[ml.mlflow]
# MLflow Model Registry Configuration
tracking_uri = "http://localhost:5001"        # MLflow tracking server URI
experiment_name = "mlb_betting_models"        # Default experiment name
artifact_store = "s3://mlb-betting-artifacts" # Optional S3 artifact storage
connection_timeout = 30                       # Connection timeout in seconds
retry_attempts = 3                           # Number of retry attempts
api_key = "${MLFLOW_API_KEY}"               # API key for authentication (env var)

[ml.redis]
# Redis Feature Store Configuration  
host = "localhost"                           # Redis server host
port = 6379                                 # Redis server port
password = "${REDIS_PASSWORD}"              # Redis password (env var)
ssl_enabled = false                         # Enable SSL/TLS encryption
ssl_cert_path = ""                          # Path to SSL certificate
ssl_key_path = ""                           # Path to SSL private key
ssl_ca_path = ""                            # Path to SSL CA certificate
connection_pool_size = 20                   # Connection pool size
socket_timeout = 5.0                        # Socket timeout in seconds
max_retries = 3                             # Maximum connection retries
retry_delay_seconds = 1.0                   # Initial retry delay in seconds
database = 0                                # Redis database number

[ml.model_thresholds]
# Model Performance Thresholds (MLB-specific with business context)

# Staging Promotion Thresholds
staging_min_accuracy = 0.55        # Above random baseline (50%) + market efficiency buffer
staging_min_roc_auc = 0.60         # Meaningful discrimination capability for betting
staging_min_precision = 0.50       # Balanced precision/recall for betting scenarios
staging_min_recall = 0.50          # Ensure we catch profitable opportunities
staging_min_training_samples = 50  # Minimum samples for statistical significance

# Production Promotion Thresholds  
production_min_accuracy = 0.60     # Higher bar for production deployment
production_min_roc_auc = 0.65      # Strong discrimination required in production
production_min_f1_score = 0.58     # Balanced performance for live betting
production_min_roi = 0.05          # 5% ROI minimum for profitability in MLB betting
production_evaluation_days = 7     # Days of staging evaluation before production

[ml.performance]
# Performance and Resource Management
memory_limit_mb = 2048              # Memory threshold for cleanup (MB)
batch_size_limit = 50               # Maximum batch processing size
connection_pool_size = 20           # Database connection pool size
feature_cache_ttl_seconds = 900     # Feature cache time-to-live (15 minutes)
model_cache_size = 10               # Maximum models to keep in memory
model_loading_timeout_seconds = 30  # Timeout for model loading operations

# Memory Management Thresholds
memory_cleanup_trigger_mb = 500     # Memory increase threshold to trigger cleanup
max_concurrent_extractions = 5      # Maximum concurrent feature extractions
dataframe_chunk_size = 1000         # Chunk size for large DataFrame operations

[ml.retraining]
# Automated Retraining Configuration
monitoring_interval_minutes = 30              # Performance monitoring interval
staging_evaluation_delay_seconds = 300       # 5 minutes delay before production promotion
performance_check_interval_hours = 1         # How often to check model performance
performance_degradation_threshold = 0.05     # 5% degradation triggers retraining
data_drift_threshold = 0.1                   # Data drift threshold for retraining
auto_retraining_enabled = true               # Enable automatic retraining
max_concurrent_retraining_jobs = 2           # Maximum concurrent retraining jobs

# Default Retraining Schedule
default_schedule_cron = "0 2 * * *"          # Daily at 2 AM
default_sliding_window_days = 90             # 90-day training window
default_min_samples = 100                    # Minimum samples for retraining

[ml.security]
# ML System Security Configuration
enable_api_authentication = true            # Enable API key authentication
api_key_header = "X-ML-API-Key"             # Header name for API key
enable_audit_logging = true                 # Enable audit logging for ML operations
audit_log_level = "INFO"                    # Audit log level
sensitive_data_masking = true               # Mask sensitive data in logs
model_encryption_enabled = false            # Enable model artifact encryption (future)

[ml.monitoring]
# ML System Monitoring and Observability  
enable_prometheus_metrics = true            # Enable Prometheus metrics collection
metrics_port = 9090                         # Port for metrics endpoint
enable_model_drift_detection = true        # Enable model drift monitoring
drift_detection_interval_hours = 6         # How often to check for drift
enable_performance_alerts = true           # Enable performance degradation alerts
alert_webhook_url = "${ML_ALERT_WEBHOOK}"   # Webhook for alerts (env var)

# Metric Collection Settings
collect_prediction_metrics = true          # Collect prediction latency/accuracy
collect_feature_metrics = true             # Collect feature extraction metrics  
collect_training_metrics = true            # Collect training pipeline metrics
metrics_retention_days = 30                # How long to retain detailed metrics