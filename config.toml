[database]
# Database file path
# path = "data/raw/mlb_betting.duckdb"  # Migrated to PostgreSQL

[schemas]
# Pipeline zone schemas
raw = "raw_data"
staging = "staging"
curated = "curated"
# Legacy schema
legacy = "splits"

[pipeline]
# Pipeline configuration
enable_staging = true
enable_curated = false  # Disabled until RAW/STAGING are stable
auto_promotion = true  # Re-enabled with zone-specific checks
validation_enabled = true
quality_threshold = 0.8  # Minimum quality score for promotion

[pipeline.zones]
raw_enabled = true
staging_enabled = true
curated_enabled = false  # Disabled until RAW/STAGING are stable

[tables]
# Main table for MLB betting splits
mlb_betting_splits = "raw_mlb_betting_splits"

# Legacy table (for backward compatibility)
legacy_splits = "splits"

[data_sources]
# Source identifiers
sbd = "SBD"  # SportsBettingDime
vsin = "VSIN"  # VSIN

[api]
# SportsBettingDime API configuration
sbd_url = "https://srfeeds.sportsbettingdime.com/v2/matchups/mlb/betting-splits"
sbd_books = ["betmgm", "bet365", "fanatics", "draftkings", "caesars", "fanduel"]

[monitoring]
# Prometheus metrics configuration
enable_metrics_sampling = true
metrics_sample_rate = 0.1  # 10% sampling for high-volume operations
max_unmapped_resolution_limit = 100  # Maximum unmapped IDs to resolve per batch

# Metric bucket configurations (in seconds)
pipeline_duration_buckets = [1, 5, 10, 30, 60, 120, 300, 600]
pipeline_stage_duration_buckets = [0.1, 0.5, 1, 2, 5, 10, 30, 60]
database_query_duration_buckets = [0.001, 0.01, 0.1, 0.5, 1, 2, 5, 10]
api_call_duration_buckets = [0.1, 0.5, 1, 2, 5, 10, 30, 60] 