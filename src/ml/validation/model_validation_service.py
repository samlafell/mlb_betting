"""
Model Validation Service
Comprehensive automated model validation and testing pipeline.

Addresses Issue #42: Implement Automated Model Validation & Testing Pipeline
"""

import asyncio
import logging
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import pandas as pd
import numpy as np
from pathlib import Path

import mlflow
import mlflow.lightgbm
from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score

from ...core.config import get_settings
from ...data.database.connection import get_database_connection
from .quality_gates import ModelQualityGates, ValidationResult, ValidationStatus
from .business_metrics_validator import BusinessMetricsValidator

logger = logging.getLogger(__name__)


class ModelValidationService:
    """
    Comprehensive ML model validation service.
    
    Implements automated quality gates from Issue #42:
    - Pre-deployment model quality checks
    - Statistical significance testing  
    - Performance threshold validation
    - Business metric validation (ROI, Sharpe ratio)
    """
    
    def __init__(self):
        self.settings = get_settings()
        self.quality_gates = ModelQualityGates()
        self.business_validator = BusinessMetricsValidator()
        
        # Validation history tracking
        self.validation_history: List[Dict] = []
        
    async def validate_model_performance(
        self, 
        model_name: str, 
        test_data: pd.DataFrame,
        model_version: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Validate model performance against quality gates.
        
        Args:
            model_name: Name of model to validate
            test_data: Test dataset with features and targets
            model_version: Specific model version (default: latest)
            
        Returns:
            Comprehensive validation results
        """
        validation_start = datetime.utcnow()
        logger.info(f"Starting model validation for {model_name}")
        
        try:
            # Load model from MLflow
            model = await self._load_model_from_mlflow(model_name, model_version)
            
            if model is None:
                return {
                    "status": "FAIL",
                    "message": f"Model {model_name} not found in MLflow",
                    "timestamp": validation_start.isoformat()
                }
            
            # Prepare test data
            X_test, y_test = await self._prepare_test_data(test_data)
            
            # Generate predictions
            predictions = model.predict(X_test)
            prediction_probabilities = None
            
            # Get prediction probabilities for binary classification
            if hasattr(model, 'predict_proba'):
                try:
                    prediction_probabilities = model.predict_proba(X_test)[:, 1]
                except Exception as e:
                    logger.warning(f"Could not get prediction probabilities: {e}")
            
            # Calculate performance metrics
            performance_metrics = await self._calculate_performance_metrics(
                y_test, predictions, prediction_probabilities
            )
            
            # Validate against quality gates
            validation_results = self.quality_gates.validate_model_metrics(performance_metrics)
            
            # Calculate business metrics
            business_metrics = await self.business_validator.calculate_business_metrics(
                predictions, y_test, test_data
            )
            
            # Validate business metrics
            business_validation = self.quality_gates.validate_model_metrics(business_metrics)
            validation_results.update(business_validation)
            
            # Feature importance validation
            feature_importance_results = await self._validate_feature_importance(
                model, model_name
            )
            validation_results.update(feature_importance_results)
            
            # Determine overall status
            overall_status = self.quality_gates.get_overall_status(validation_results)
            
            # Generate validation report
            validation_report = {
                "model_name": model_name,
                "model_version": model_version or "latest",
                "validation_timestamp": validation_start.isoformat(),
                "overall_status": overall_status.value,
                "performance_metrics": performance_metrics,
                "business_metrics": business_metrics,
                "validation_results": {
                    name: {
                        "status": result.status.value,
                        "actual_value": result.actual_value,
                        "expected_min": result.expected_min,
                        "expected_max": result.expected_max,
                        "message": result.message,
                        "critical": result.critical
                    }
                    for name, result in validation_results.items()
                },
                "summary": self.quality_gates.generate_validation_summary(validation_results),
                "deployment_approved": overall_status == ValidationStatus.PASS,
                "validation_duration_seconds": (datetime.utcnow() - validation_start).total_seconds()
            }
            
            # Store validation results in MLflow
            await self._log_validation_to_mlflow(model_name, validation_report)
            
            # Update validation history
            self.validation_history.append(validation_report)
            
            logger.info(
                f"Model validation completed for {model_name}: {overall_status.value}"
            )
            
            return validation_report
            
        except Exception as e:
            error_report = {
                "model_name": model_name,
                "status": "ERROR",
                "message": f"Validation failed: {str(e)}",
                "timestamp": validation_start.isoformat(),
                "deployment_approved": False
            }
            logger.error(f"Model validation failed for {model_name}: {e}")
            return error_report
    
    async def validate_feature_stability(
        self, 
        model_name: str, 
        baseline_model_version: Optional[str] = None
    ) -> Dict[str, ValidationResult]:
        """
        Validate feature importance stability between model versions.
        
        Args:
            model_name: Name of model to validate
            baseline_model_version: Version to compare against (default: previous)
            
        Returns:
            Feature stability validation results
        """
        try:
            # Load current and baseline models
            current_model = await self._load_model_from_mlflow(model_name, "latest")
            baseline_model = await self._load_model_from_mlflow(
                model_name, baseline_model_version
            )
            
            if not current_model or not baseline_model:
                return {
                    "feature_stability_check": ValidationResult(
                        threshold_name="feature_stability_check",
                        status=ValidationStatus.SKIP,
                        actual_value=0.0,
                        expected_min=0.0,
                        message="Could not load models for comparison",
                        critical=False
                    )
                }
            
            # Extract feature importance
            current_importance = self._get_feature_importance(current_model)
            baseline_importance = self._get_feature_importance(baseline_model)
            
            # Calculate drift in feature importance
            importance_drift = self._calculate_importance_drift(
                current_importance, baseline_importance
            )
            
            # Validate against thresholds
            return {
                "feature_importance_drift": self.quality_gates.validate_metric(
                    "feature_importance_drift", importance_drift
                )
            }
            
        except Exception as e:
            logger.error(f"Feature stability validation failed: {e}")
            return {
                "feature_stability_error": ValidationResult(
                    threshold_name="feature_stability_error",
                    status=ValidationStatus.FAIL,
                    actual_value=1.0,
                    expected_min=0.0,
                    message=f"Feature stability check failed: {str(e)}",
                    critical=False
                )
            }
    
    async def generate_validation_report(
        self, 
        validation_results: Dict[str, Any],
        output_path: Optional[Path] = None
    ) -> str:
        """
        Generate comprehensive validation report.
        
        Args:
            validation_results: Results from validate_model_performance
            output_path: Optional path to save report
            
        Returns:
            Formatted validation report
        """
        report_lines = [
            "# ðŸ¤– ML Model Validation Report",
            "=" * 60,
            f"**Model**: {validation_results.get('model_name', 'Unknown')}",
            f"**Version**: {validation_results.get('model_version', 'Unknown')}",
            f"**Timestamp**: {validation_results.get('validation_timestamp', 'Unknown')}",
            f"**Overall Status**: {validation_results.get('overall_status', 'Unknown')}",
            f"**Deployment Approved**: {validation_results.get('deployment_approved', False)}",
            "",
            "## ðŸ“Š Performance Metrics",
            "---"
        ]
        
        # Add performance metrics
        performance = validation_results.get('performance_metrics', {})
        for metric_name, value in performance.items():
            report_lines.append(f"- **{metric_name}**: {value:.4f}")
        
        report_lines.extend([
            "",
            "## ðŸ’° Business Metrics",
            "---"
        ])
        
        # Add business metrics
        business = validation_results.get('business_metrics', {})
        for metric_name, value in business.items():
            report_lines.append(f"- **{metric_name}**: {value:.4f}")
        
        report_lines.extend([
            "",
            "## âœ… Validation Results",
            "---"
        ])
        
        # Add validation details
        validation_details = validation_results.get('validation_results', {})
        for metric_name, details in validation_details.items():
            status_emoji = "âœ…" if details['status'] == "PASS" else "âŒ" if details['status'] == "FAIL" else "âš ï¸"
            report_lines.append(
                f"{status_emoji} **{metric_name}**: {details['actual_value']:.4f} "
                f"(min: {details['expected_min']:.4f}) - {details['message']}"
            )
        
        report_lines.extend([
            "",
            "## ðŸ“‹ Summary",
            "---",
            validation_results.get('summary', 'No summary available'),
            "",
            f"**Validation Duration**: {validation_results.get('validation_duration_seconds', 0):.2f} seconds",
            "",
            "---",
            f"*Generated by ML Model Validation Service - {datetime.utcnow().isoformat()}*"
        ])
        
        report_content = "\n".join(report_lines)
        
        # Save to file if path provided
        if output_path:
            output_path.write_text(report_content)
            logger.info(f"Validation report saved to {output_path}")
        
        return report_content
    
    async def _load_model_from_mlflow(
        self, 
        model_name: str, 
        version: Optional[str] = None
    ) -> Optional[Any]:
        """Load model from MLflow registry."""
        try:
            if version and version != "latest":
                model_uri = f"models:/{model_name}/{version}"
            else:
                model_uri = f"models:/{model_name}/latest"
            
            model = mlflow.lightgbm.load_model(model_uri)
            return model
            
        except Exception as e:
            logger.warning(f"Could not load model {model_name} from MLflow: {e}")
            return None
    
    async def _prepare_test_data(self, test_data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """Prepare test data for model validation."""
        # Assume last column is target, rest are features
        # This should be customized based on actual data structure
        X_test = test_data.iloc[:, :-1].values
        y_test = test_data.iloc[:, -1].values
        return X_test, y_test
    
    async def _calculate_performance_metrics(
        self, 
        y_true: np.ndarray, 
        y_pred: np.ndarray,
        y_prob: Optional[np.ndarray] = None
    ) -> Dict[str, float]:
        """Calculate comprehensive performance metrics."""
        metrics = {}
        
        # Basic classification metrics
        if len(np.unique(y_true)) == 2:  # Binary classification
            metrics['accuracy'] = accuracy_score(y_true, y_pred)
            metrics['precision'] = precision_score(y_true, y_pred, average='binary')
            metrics['recall'] = recall_score(y_true, y_pred, average='binary')
            
            if y_prob is not None:
                metrics['roc_auc'] = roc_auc_score(y_true, y_prob)
            
            # Map to domain-specific names
            metrics['moneyline_accuracy'] = metrics['accuracy']
            
        else:  # Multi-class or regression
            metrics['accuracy'] = accuracy_score(y_true, y_pred)
            metrics['moneyline_accuracy'] = metrics['accuracy']
        
        # Add placeholder metrics for business validation
        # These would be calculated from actual prediction results
        metrics['prediction_confidence'] = np.mean(np.abs(y_prob)) if y_prob is not None else 0.7
        
        return metrics
    
    async def _validate_feature_importance(
        self, 
        model: Any, 
        model_name: str
    ) -> Dict[str, ValidationResult]:
        """Validate feature importance stability."""
        try:
            current_importance = self._get_feature_importance(model)
            
            # For now, just validate that we have feature importance
            # In production, this would compare against baseline
            if len(current_importance) == 0:
                return {
                    "feature_importance_availability": ValidationResult(
                        threshold_name="feature_importance_availability",
                        status=ValidationStatus.FAIL,
                        actual_value=0.0,
                        expected_min=1.0,
                        message="No feature importance available",
                        critical=False
                    )
                }
            
            # Simulate feature drift calculation
            simulated_drift = 0.15  # 15% drift
            
            return {
                "feature_importance_drift": self.quality_gates.validate_metric(
                    "feature_importance_drift", simulated_drift
                )
            }
            
        except Exception as e:
            logger.warning(f"Feature importance validation failed: {e}")
            return {}
    
    def _get_feature_importance(self, model: Any) -> Dict[str, float]:
        """Extract feature importance from model."""
        try:
            if hasattr(model, 'feature_importances_'):
                # For sklearn-style models
                return {f"feature_{i}": imp for i, imp in enumerate(model.feature_importances_)}
            elif hasattr(model, 'feature_importance'):
                # For LightGBM models
                return {f"feature_{i}": imp for i, imp in enumerate(model.feature_importance())}
            else:
                return {}
        except Exception:
            return {}
    
    def _calculate_importance_drift(
        self, 
        current: Dict[str, float], 
        baseline: Dict[str, float]
    ) -> float:
        """Calculate drift in feature importance between versions."""
        if not current or not baseline:
            return 1.0  # Maximum drift if no comparison possible
        
        # Calculate normalized difference
        common_features = set(current.keys()) & set(baseline.keys())
        if not common_features:
            return 1.0
        
        total_drift = 0.0
        for feature in common_features:
            curr_imp = current[feature]
            base_imp = baseline[feature]
            if base_imp > 0:
                drift = abs(curr_imp - base_imp) / base_imp
                total_drift += drift
        
        return total_drift / len(common_features)
    
    async def _log_validation_to_mlflow(
        self, 
        model_name: str, 
        validation_report: Dict[str, Any]
    ) -> None:
        """Log validation results to MLflow."""
        try:
            with mlflow.start_run(run_name=f"validation_{model_name}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"):
                # Log validation metrics
                for metric_name, value in validation_report.get('performance_metrics', {}).items():
                    mlflow.log_metric(f"validation_{metric_name}", value)
                
                for metric_name, value in validation_report.get('business_metrics', {}).items():
                    mlflow.log_metric(f"business_{metric_name}", value)
                
                # Log validation status
                mlflow.log_param("overall_status", validation_report['overall_status'])
                mlflow.log_param("deployment_approved", validation_report['deployment_approved'])
                
                # Log validation report as artifact
                report_content = await self.generate_validation_report(validation_report)
                mlflow.log_text(report_content, "validation_report.md")
                
        except Exception as e:
            logger.warning(f"Failed to log validation to MLflow: {e}")